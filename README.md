Credit Card Fraud Detection - Machine Learning Project


This project is a comprehensive hands-on machine learning pipeline to detect fraudulent credit card transactions. It includes data exploration, feature engineering, model building, evaluation, cross-validation, and class imbalance handling techniques.
üî¢ Project Goal
Detect fraudulent credit card transactions using supervised machine learning, helping financial institutions proactively prevent fraud.
________________________________________
üß¨ Skills and Tools Used
‚Ä¢	Language: Python
‚Ä¢	Environment: Google Colab (Jupyter Notebook)
‚Ä¢	Libraries:
o	Data Analysis: NumPy, Pandas
o	Visualization: Matplotlib, Seaborn
o	Modeling: Scikit-learn, XGBoost
o	Evaluation: SciPy, Statsmodels
________________________________________
üìÖ Dataset Overview
‚Ä¢	Source: European cardholder transaction dataset from September 2013
‚Ä¢	Size: 284,807 transactions
‚Ä¢	Fraudulent: 492 transactions (~0.172%)
‚Ä¢	Features:
o	Time (seconds elapsed since first transaction)
o	Amount
o	V1 to V28: Principal components from PCA (feature anonymization)
o	Class: Target variable (1 = Fraud, 0 = Legitimate)
________________________________________
üí° Project Steps
1. Data Understanding & EDA
‚Ä¢	Load and inspect the data: .head(), .shape(), .info(), .describe()
‚Ä¢	Analyze class imbalance using bar/pie plots
‚Ä¢	Compute and visualize feature correlations via heatmaps
2. Feature Engineering
‚Ä¢	Transform Time into time_day, time_hour, time_minute
‚Ä¢	Drop irrelevant columns, retain time_hour
3. Train-Test Split
‚Ä¢	80% Training / 20% Testing using train_test_split
4. Visualization of Feature Distributions
‚Ä¢	Plots (histograms) show skewness and class-based variation
________________________________________
üìä Evaluation Metrics
‚Ä¢	Confusion Matrix: TP, FP, TN, FN breakdown
‚Ä¢	Classification Report: Precision, Recall, F1-score
‚Ä¢	AUC-ROC: Area under curve for model discrimination performance
________________________________________
üìà Model Building & Evaluation
Reusable functions were built for training and evaluating each model type:
Models Used
‚Ä¢	Logistic Regression (L1 & L2 Regularization)
‚Ä¢	K-Nearest Neighbors (KNN)
‚Ä¢	Decision Trees (gini, entropy)
‚Ä¢	Random Forest
‚Ä¢	Support Vector Machine (SVM) (sigmoid kernel)
‚Ä¢	XGBoost
Each model is evaluated using: - Accuracy - ROC AUC - Optimal threshold (from ROC curve) - Confusion matrix and ROC plot
All results are stored in a unified df_results DataFrame.
________________________________________
üìä Methodologies Applied
1. Repeated K-Fold Cross-Validation
‚Ä¢	Ensures robustness through multiple splits
‚Ä¢	Logistic Regression with L2 performs best
2. Stratified K-Fold Cross-Validation
‚Ä¢	Maintains class balance across folds
‚Ä¢	Logistic Regression with L2 again outperforms others
3. Hyperparameter Tuning
‚Ä¢	Best Base Model: Logistic Regression with L2 (Stratified K-Fold)
‚Ä¢	Tuned parameters: C, solver, tolerance
‚Ä¢	Feature importances (coefficients) are plotted
________________________________________
üéØ Handling Class Imbalance
Applied oversampling techniques with Stratified K-Fold:
Techniques:
‚Ä¢	Random Over Sampler
‚Ä¢	SMOTE (Synthetic Minority Over-sampling)
‚Ä¢	ADASYN (Adaptive Synthetic Sampling)
Results:
‚Ä¢	XGBoost performed best for all oversampling methods
________________________________________
‚öñÔ∏è Final Hyperparameter Tuning
‚Ä¢	Model: XGBoost + Random Over Sampling + Stratified K-Fold
‚Ä¢	Tuned using RandomizedSearchCV
‚Ä¢	Parameters tuned: max_depth, min_child_weight, n_estimators, learning_rate
‚Ä¢	Feature importance plotted from final model
________________________________________
üåü Conclusion
‚Ä¢	Best Base Model: Logistic Regression with L2 (Stratified K-Fold)
‚Ä¢	Best Oversampled Model: XGBoost with Random Over Sampler (Stratified K-Fold)
This project showcases a complete ML workflow from EDA to advanced modeling and tuning, offering a powerful template for fraud detection problems.
________________________________________
üìÇ Files Included
‚Ä¢	credit_card_fraud_detection.ipynb: Full notebook with code and explanations
‚Ä¢	creditcard.csv: Dataset (should be uploaded separately)
‚Ä¢	requirements.txt: List of required packages
________________________________________
üìç How to Run
1.	Clone the repo or upload the notebook to Google Colab
2.	Upload the dataset
3.	Run cells step-by-step to execute the pipeline
________________________________________
‚úàÔ∏è Future Improvements
‚Ä¢	Try neural networks or autoencoders
‚Ä¢	Apply cost-sensitive learning
‚Ä¢	Deploy as a REST API using FastAPI or Flask
________________________________________
üåê Acknowledgments
Dataset provided by Kaggle - Credit Card Fraud Detection
________________________________________
üìó References
‚Ä¢	Scikit-learn Documentation
‚Ä¢	XGBoost Documentation
‚Ä¢	Imbalanced-learn (SMOTE, ADASYN)
‚Ä¢	Seaborn & Matplotlib Docs
________________________________________
‚ÄúEven with highly imbalanced data, smart preprocessing and model tuning can yield strong results.‚Äù
________________________________________
Author: Maniska Biswas
linkedIn: www.linkedin.com/in/maniska-biswas
